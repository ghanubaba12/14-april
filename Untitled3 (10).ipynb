{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35aacc-6d7a-4d66-9615-62340c185236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved.\n",
    "Dataset link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?\n",
    "usp=share_link\n",
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "ans-To preprocess the dataset, we will follow these steps:\n",
    "\n",
    "Load the dataset\n",
    "Check for missing values and handle them if present\n",
    "Encode categorical variables\n",
    "Scale the numerical features\n",
    "Here's the code to preprocess the dataset:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# There are no missing values in the dataset\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "le = LabelEncoder()\n",
    "for var in categorical_vars:\n",
    "    df[var] = le.fit_transform(df[var].astype('str'))\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_vars] = scaler.fit_transform(df[numerical_vars])\n",
    "\n",
    "# Print the preprocessed dataset\n",
    "print(df.head())\n",
    "In this code, we first load the dataset using the read_csv function from pandas. We then check for missing values using the isnull() function and find that there are no missing values in the dataset.\n",
    "\n",
    "Next, we encode the categorical variables using the LabelEncoder class from scikit-learn. We loop over each categorical variable, convert it to string type, and fit the LabelEncoder on it to transform the variable into a numerical format.\n",
    "\n",
    "Finally, we scale the numerical features using the StandardScaler class from scikit-learn. We select the numerical features, apply the fit_transform method to them, and update the dataframe with the scaled values.\n",
    "\n",
    "The preprocessed dataset is printed using the head() method to check if everything has been processed correctly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579341c-a8da-4f90-8ed4-cabc8e3792bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "ans-Sure, I can provide you with an example code for splitting a dataset into a training set and a test set using Python and the scikit-learn library. Here is an example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is the feature matrix and y is the target variable array\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Here, X_train and y_train will be the training set and X_test and y_test will be the test set\n",
    "In this code, train_test_split function from the model_selection module of the scikit-learn library is used to split the dataset into a training set and a test set. The function takes two arrays: X, which contains the feature matrix, and y, which contains the target variable. The test_size parameter is set to 0.3, which means that 30% of the data will be used for testing, and 70% will be used for training. The random_state parameter is set to 42 to ensure that the data is split in a reproducible way.\n",
    "\n",
    "After splitting the data, the resulting arrays X_train, X_test, y_train, and y_test contain the training and test sets, respectively. These arrays can be used as input to train and evaluate machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e7e24-95fb-4803-87a9-4ffc9967a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "To train a random forest classifier on the preprocessed dataset, we will first split the dataset into training and testing sets. We will use 80% of the data for training and 20% for testing. We will then use the RandomForestClassifier class from scikit-learn to train a random forest classifier with 100 trees and a maximum depth of 10 for each tree.\n",
    "\n",
    "Here's the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier with 100 trees and max depth of 10\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "In this code, we first split the preprocessed dataset into training and testing sets using the train_test_split function from scikit-learn. We then initialize a RandomForestClassifier object with 100 trees and a maximum depth of 10 for each tree.\n",
    "\n",
    "Next, we fit the random forest classifier on the training set using the fit method. We then use the predict method to predict the target variable for the test set.\n",
    "\n",
    "Finally, we calculate the accuracy of the model on the test set using the score method and print it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657838b3-8dd9-4de1-b010-958ff6df7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "ans-To evaluate the performance of the trained random forest classifier on the test set, we can use various performance metrics such as accuracy, precision, recall, and F1 score. We can use the scikit-learn's classification_report and confusion_matrix functions to compute these metrics.\n",
    "\n",
    "Here's the code to compute the metrics:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Compute the performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "In this code, we first predict the target variable for the test set using the predict method of the trained random forest classifier.\n",
    "\n",
    "We then use the confusion_matrix function from scikit-learn to compute the confusion matrix for the predictions. The confusion matrix is a table that shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "Next, we use the classification_report function to compute precision, recall, F1 score, and support for both classes (0 and 1) in the test set.\n",
    "\n",
    "The output of this code will show the confusion matrix and classification report.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e40309-0ffc-4d76-a9b3-076049270787",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "ans-Sure, here's an example code to identify the top 5 most important features in predicting heart disease risk and visualise the feature importances using a bar chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming X is the feature matrix and y is the target variable array\n",
    "# Initialize the Random Forest Classifier with n_estimators=100 and random_state=42\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the entire dataset\n",
    "rfc.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = rfc.feature_importances_\n",
    "\n",
    "# Get the indices of the top 5 features\n",
    "top_5_indices = np.argsort(feature_importances)[-5:]\n",
    "\n",
    "# Get the names of the top 5 features\n",
    "top_5_features = X.columns[top_5_indices]\n",
    "\n",
    "# Plot the feature importances as a bar chart\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(top_5_features, feature_importances[top_5_indices])\n",
    "plt.title(\"Top 5 Most Important Features for Predicting Heart Disease Risk\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "In this example code, we first initialize a Random Forest Classifier with n_estimators=100 and random_state=42. We then train the classifier on the entire dataset X and y.\n",
    "\n",
    "We then obtain the feature importances using the feature_importances_ attribute of the trained Random Forest Classifier. We use NumPy's argsort() function to obtain the indices of the top 5 features with the highest feature importances.\n",
    "\n",
    "We then obtain the names of the top 5 features using the columns attribute of the feature matrix X. Finally, we plot the top 5 most important features as a bar chart using Matplotlib.\n",
    "\n",
    "Note that this is just an example, and you will need to adjust the code according to your specific dataset and requirements.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33660cb3-4687-45c0-b292-668c89f0de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "ans-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89854708-cc57-44f3-a641-e2b5492ed04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\n",
    "ans-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd2826-1012-471f-99e1-de5c0731f7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc3e29-b1fb-48a1-9379-1e0044db230c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c32477-545c-4100-8e99-ca43077613a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad86cc-dce6-400b-b49c-d2dd3e5c29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dc4a5-0dd2-4685-a4d4-18da888a9fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
